{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFeQcg9uhjDO"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUu86p1Or1n"
      },
      "source": [
        "# Prediction-Based Word Vectors\n",
        "\n",
        "more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpYg_7pODYJ",
        "outputId": "3bd347e5-2733-421f-a544-15d54009959d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import pprint\n",
        "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Words with Multiple Meanings\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZAr09U-xSSuT"
      },
      "outputs": [],
      "source": [
        "### CODE HERE\n",
        "\n",
        "def get_most_similar_words(word):\n",
        "    similar_words = wv_from_bin.most_similar(word, topn=10)\n",
        "    return [word for word in similar_words]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word1=\"mouse\"\n",
        "get_most_similar_words(word1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhyD5sN68ycw",
        "outputId": "d09e6c6b-69aa-4cbb-de73-f5bfd47f4d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mice', 0.6580958962440491),\n",
              " ('keyboard', 0.5548278093338013),\n",
              " ('rat', 0.5433949828147888),\n",
              " ('rabbit', 0.5192376971244812),\n",
              " ('cat', 0.5077415704727173),\n",
              " ('cursor', 0.5058691501617432),\n",
              " ('trackball', 0.5048902630805969),\n",
              " ('joystick', 0.49841049313545227),\n",
              " ('mickey', 0.47242844104766846),\n",
              " ('clicks', 0.4722806215286255)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2=\"paper\"\n",
        "get_most_similar_words(word2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JftGntVs-iYx",
        "outputId": "af4b31f7-5a1c-4e1a-e66a-82f63a5c4a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('newspaper', 0.671421229839325),\n",
              " ('papers', 0.6713257431983948),\n",
              " ('printed', 0.6686532497406006),\n",
              " ('sheet', 0.6124283671379089),\n",
              " ('printing', 0.6033082604408264),\n",
              " ('newspapers', 0.5930173397064209),\n",
              " ('print', 0.5892402529716492),\n",
              " ('piece', 0.5870198607444763),\n",
              " ('published', 0.581505298614502),\n",
              " ('book', 0.5597691535949707)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word3=\"run\"\n",
        "get_most_similar_words(word3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sVfX4pa-pH8",
        "outputId": "92f6008f-b745-463c-dea0-3faca28999c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('running', 0.7378345727920532),\n",
              " ('runs', 0.7364052534103394),\n",
              " ('ran', 0.696038544178009),\n",
              " ('went', 0.6395289897918701),\n",
              " ('start', 0.637183427810669),\n",
              " ('allowed', 0.6334168314933777),\n",
              " ('out', 0.6328096389770508),\n",
              " ('go', 0.6265833377838135),\n",
              " ('going', 0.6221196055412292),\n",
              " ('first', 0.6087011098861694)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word4=\"book\"\n",
        "get_most_similar_words(word4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN0Rzxrn-weM",
        "outputId": "434c28b9-6246-453e-a6a8-2aa69374b8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('books', 0.8452467918395996),\n",
              " ('author', 0.7746455669403076),\n",
              " ('novel', 0.7485204935073853),\n",
              " ('published', 0.7451642751693726),\n",
              " ('memoir', 0.7047821283340454),\n",
              " ('wrote', 0.6971326470375061),\n",
              " ('written', 0.6967507004737854),\n",
              " ('essay', 0.6844283938407898),\n",
              " ('biography', 0.681260347366333),\n",
              " ('autobiography', 0.6770558953285217)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Possible reasons:\n",
        "* Semantic Distance: In the embedding space, unrelated terms may appear more similar due to the meanings being too dissimilar.\n",
        "* Data Bias: It is possible that the word embedding model was trained on a corpus in which a single meaning of the word predominates, thereby introducing a bias into the embeddings.\n",
        "* Ambiguity: Certain terms may possess meanings that are ambiguous and therefore difficult to represent with a single vector.\n",
        "* Model Limitations: The GloVe model might not be fine-tuned to capture the subtle semantic nuances required for disambiguating meanings in certain words.\n"
      ],
      "metadata": {
        "id": "2PtxSUUv__Bm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwlpPjpHSSuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e485c013-b6d4-4b13-cf9f-08aba6d618ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms fast, quick have cosine distance: 0.3328641653060913\n",
            "Antonyms fast, slow have cosine distance: 0.2522680163383484\n"
          ]
        }
      ],
      "source": [
        "w1 = \"fast\"\n",
        "w2 = \"quick\"\n",
        "w3 = \"slow\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "because antonyms are about the same thing but in a reverse way. but synonym usually used in different situations. and I think if in a sentence we replace for example \"sad\" and \"happy\" it will be ok just the meaning will reverse but if we replace \"cheerful\" and \"happy\" may be it would not fit the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Analogies with Word Vectors\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0pC7H4VSSuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a401724-db13-4b00-d615-a5bc44dcb639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('grandmother', 0.7608445286750793),\n",
            " ('granddaughter', 0.7200808525085449),\n",
            " ('daughter', 0.7168302536010742),\n",
            " ('mother', 0.7151536345481873),\n",
            " ('niece', 0.7005682587623596),\n",
            " ('father', 0.6659887433052063),\n",
            " ('aunt', 0.6623408794403076),\n",
            " ('grandson', 0.6618767976760864),\n",
            " ('grandparents', 0.644661009311676),\n",
            " ('wife', 0.6445354223251343)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the answer, respectively. Using **only** vectors $m$, $g$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, to what expression are we maximizing $x$'s cosine similarity?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `grandfather` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "### SOLUTION\n",
        "We are trying to find a vector 洧논 such that it maximizes the cosine similarity with the result of the operation ('woman' + 'grandfather' - 'man').\n",
        "\n",
        "Let's denote the resulting vector as 洧논 = 'woman' + 'grandfather' - 'man'.\n",
        "\n",
        "So, the expression for maximizing 洧논's cosine similarity is:\n",
        "\n",
        "* 洧논 = 洧녻 + 洧녮 - 洧녴\n",
        "\n",
        "This expression represents the vector operation where we add the vector for 'woman' and 'grandfather', and then subtract the vector for 'man'. The resulting vector 洧논 is expected to have a high cosine similarity with the target word.\n",
        "\n",
        "When we compute the cosine similarity between 洧논 and other word vectors, we are essentially finding words that are semantically similar to the idea of 'woman' and 'grandfather' but different from 'man'. This is in line with the expected results of the word embedding model, where words like 'grandmother', 'daughter', 'mother', etc., are returned as similar words.\n",
        "\n",
        "#2D vector\n",
        "\n",
        "Let's consider a simplified 2D example to visualize the vectors. Assume we have a two-dimensional space where each word vector is represented by a point. For simplicity, we'll represent the vectors for 'man', 'woman', 'grandfather', and the answer ('x') on a coordinate plane.\n",
        "\n",
        "Let's arbitrarily position these vectors:<br>\n",
        "\n",
        "'man' at point M(1, 1)<br>\n",
        "'woman' at point W(3, 2)<br>\n",
        "'grandfather' at point G(2, 5)<br>\n",
        "To find the answer vector, we perform the operation: 洧논 = 洧녻 + 洧녮 - 洧녴.\n",
        "\n",
        "Since 'man' is at point M(1, 1), 'woman' is at point W(3, 2), and 'grandfather' is at point G(2, 5), let's calculate the new vector 'x':\n",
        "\n",
        "洧논 = 洧녻 + 洧녮 - 洧녴\n",
        "= (3, 2) + (2, 5) - (1, 1)\n",
        "= (3 + 2 - 1, 2 + 5 - 1)\n",
        "= (4, 6)\n",
        "\n",
        "So, the resulting vector 'x' lies at point X(4, 6).\n",
        "\n",
        "Now, if we draw the vectors on a 2D coordinate plane, the vector 'x' (representing the answer) is located relative to 'woman' and 'grandfather', which intuitively corresponds to the idea of a word similar to 'grandfather' and 'woman' but different from 'man'.<br>\n",
        "Also by using the `most_similar` function with $x$ as the input, we can find the word that has the highest cosine similarity and is most similar to the vector (4,6).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Finding Analogies\n",
        "a. For the previous example, it's clear that \"grandmother\" completes the analogy. But give an intuitive explanation as to why the `most_similar` function gives us words like \"granddaughter\", \"daughter\", or \"mother?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cause they are all in female catagory so its more likely to happend together.<br>\n",
        "Proximity in Vector Space: In the word embedding space, vectors for words related to family relations might be clustered together. Since \"granddaughter\", \"daughter\", and \"mother\" are all related to family and share certain semantic similarities with \"grandfather\", \"woman\", and \"man\", respectively, they might be close to the expected answer vector."
      ],
      "metadata": {
        "id": "Exr5LQqzRNoY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "b. Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhzQJMYYVSjf"
      },
      "outputs": [],
      "source": [
        "x, y, a, b = \"waitress\",\"waiter\",\"female\",\"male\"\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "### SOLUTION\n",
        "waiter + female - waitress = male"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Incorrect Analogy\n",
        "a. Below, we expect to see the intended analogy \"hand : glove :: foot : **sock**\", but we see an unexpected result instead. Give a potential reason as to why this particular analogy turned out the way it did?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ykWoJoSSuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e8cd12-64f4-40aa-d9bc-44e539dd297c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "### SOLUTION\n",
        "I think maybe in a input model that trained these words used rarly or they use in a context with different meanings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "b. Find another example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the **incorrect** value of b according to the word vectors (in the previous example, this would be **'45,000-square'**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "D_rlci42XQTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e01d9c-a3ff-4a77-8cdb-644df9f9b7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('sky', 0.5087384581565857),\n",
            " ('bright', 0.46601271629333496),\n",
            " ('moon', 0.4504569172859192),\n",
            " ('jiazheng', 0.44230249524116516),\n",
            " ('shine', 0.43356406688690186),\n",
            " ('shines', 0.43294209241867065),\n",
            " ('earth', 0.4307621717453003),\n",
            " ('solar', 0.4234519898891449),\n",
            " ('blue', 0.420571506023407),\n",
            " ('light', 0.41411274671554565)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x, y, a, b = \"white\", \"black\", \"sun\", \"moon\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "### SOLUTION\n",
        "sun + black - white <br>\n",
        "I expected the model to notice the contrast in the first pair and return the word \"moon\" instead of \"sky\"!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Guided Analysis of Bias in Word Vectors\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XggWA4MhSSuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cf694e-c7b6-49e4-b621-a1fc2ba45616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n",
            "\n",
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be most dissimilar from.\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "Most similar word to man is \"reputation\"! and the most similar word to woman is \"professions\". The words similar to man are more abstract like skill, regarded while the words similar to woman are more practical like teacher, nursing.\n",
        "The gender bias in the word vectors is evident in the difference between the lists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Independent Analysis of Bias in Word Vectors\n",
        "\n",
        "Use the `most_similar` function to find another pair of analogies that demonstrates some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZoDheIfSSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c47ee89-1384-451b-f6ca-6b43ae83f0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('workers', 0.611325740814209),\n",
            " ('employee', 0.5983108878135681),\n",
            " ('working', 0.5615329742431641),\n",
            " ('laborer', 0.5442320108413696),\n",
            " ('unemployed', 0.536851704120636),\n",
            " ('job', 0.5278826355934143),\n",
            " ('work', 0.5223963856697083),\n",
            " ('mechanic', 0.5088937282562256),\n",
            " ('worked', 0.5054520964622498),\n",
            " ('factory', 0.4940454363822937)]\n",
            "\n",
            "[('employee', 0.6375863552093506),\n",
            " ('workers', 0.6068920493125916),\n",
            " ('nurse', 0.5837947130203247),\n",
            " ('pregnant', 0.5363885164260864),\n",
            " ('mother', 0.5321308970451355),\n",
            " ('employer', 0.5127025842666626),\n",
            " ('teacher', 0.5099576711654663),\n",
            " ('child', 0.5096741318702698),\n",
            " ('homemaker', 0.5019454956054688),\n",
            " ('nurses', 0.4970572590827942)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "A =\"man\"\n",
        "B =\"woman\"\n",
        "word =\"worker\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "### SOLUTION\n",
        "The words that relate to man are all about work and job while the words relate to woman has some words like pregnant, child and homemaker in the concept of job!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another example\n",
        "A = \"small\"\n",
        "B = \"mouse\"\n",
        "word = \"tiny\"\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQH_4b2HsOba",
        "outputId": "f2689e68-a4b8-431f-e9d6-b0469463ec6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('large', 0.680227518081665),\n",
            " ('smaller', 0.623263418674469),\n",
            " ('larger', 0.551127016544342),\n",
            " ('few', 0.541064441204071),\n",
            " ('huge', 0.526732325553894),\n",
            " ('sized', 0.515216588973999),\n",
            " ('relatively', 0.5117579698562622),\n",
            " ('sizable', 0.5114010572433472),\n",
            " ('nearby', 0.5099323987960815),\n",
            " ('largest', 0.5029460787773132)]\n",
            "\n",
            "[('mice', 0.5267527103424072),\n",
            " ('trackball', 0.4856228232383728),\n",
            " ('cursor', 0.47818639874458313),\n",
            " ('keyboard', 0.46513333916664124),\n",
            " ('white-footed', 0.45868802070617676),\n",
            " ('joystick', 0.4565153419971466),\n",
            " ('microcebus', 0.4375380277633667),\n",
            " ('reepicheep', 0.43316853046417236),\n",
            " ('peromyscus', 0.4318021237850189),\n",
            " ('rabbit', 0.43032732605934143)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another example\n",
        "A = \"running\"\n",
        "B = \"march\"\n",
        "word = \"spring\"\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVTX6KN_xBRG",
        "outputId": "4d2c7816-a2c4-426c-d616-86345195d3d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('run', 0.5648393630981445),\n",
            " ('plenty', 0.5220548510551453),\n",
            " ('ran', 0.5006933212280273),\n",
            " ('catching', 0.4874459505081177),\n",
            " ('always', 0.4865102767944336),\n",
            " ('winter', 0.4798910617828369),\n",
            " ('summer', 0.4785727858543396),\n",
            " ('runs', 0.47564056515693665),\n",
            " ('start', 0.46902260184288025),\n",
            " ('like', 0.4679262638092041)]\n",
            "\n",
            "[('july', 0.7441548109054565),\n",
            " ('april', 0.7351762056350708),\n",
            " ('june', 0.7311972975730896),\n",
            " ('september', 0.7232730388641357),\n",
            " ('october', 0.7174134850502014),\n",
            " ('august', 0.7155402302742004),\n",
            " ('february', 0.7122297883033752),\n",
            " ('january', 0.7020894885063171),\n",
            " ('december', 0.7004657983779907),\n",
            " ('november', 0.6892157196998596)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "when spring and running come together, there are verbs related to run and start but when it is used with march, the other months of sprint will be most related"
      ],
      "metadata": {
        "id": "60WmNEMnx_Yc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Thinking About Bias\n",
        "\n",
        "a. Give one explanation of how bias gets into the word vectors. Briefly describe a real-world example that demonstrates this source of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "it is what glove embeddings do and its also related to the input database and how it is trained.\n",
        "\n",
        "Word embeddings are typically trained on large corpora of text, which often reflect the biases and stereotypes present in society. As a result, the patterns learned by the model during training may encode and reinforce these biases in the resulting word vectors.\n",
        "\n",
        "For example, consider a word embedding model trained on news articles from various sources. If certain demographics are overrepresented or underrepresented in these articles, the model may learn biased associations between words and concepts. For instance, if news articles predominantly mention men in positions of power and women in domestic roles, the resulting word vectors may reflect and reinforce these gender stereotypes. As a consequence, words like \"doctor\" might be more closely associated with male pronouns, while words like \"nurse\" might be more closely associated with female pronouns, even though these professions are not inherently gendered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "b. What is one method you can use to mitigate bias exhibited by word vectors?  Briefly describe a real-world example that demonstrates this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "\n",
        "### SOLUTION\n",
        "\n",
        " One approach is modifying the word vectors to reduce or eliminate biased correlations, such as gender or racial stereotypes. For example, debiasing algorithms may identify gender-neutral words and adjust their embeddings to be equidistant from gendered words like \"he\" and \"she.\" By doing so, the resulting word vectors are less likely to reflect biased associations present in the training data. A real-world example of this method is the work done by Bolukbasi et al. (2016), who proposed a debiasing algorithm to mitigate gender bias in word embeddings. Their algorithm identified gender-specific word pairs and applied transformations to the word vectors to reduce the gender bias while preserving the semantic relationships between words. This approach has been applied to various natural language processing tasks to promote fairness and reduce discrimination in machine learning systems.<br>\n",
        " article link:https://arxiv.org/abs/1607.06520"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}